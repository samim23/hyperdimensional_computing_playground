{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "fmhH3USYnPws"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPt6rOB8nokiXKG+2jT+kCA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samim23/hyperdimensional_computing_playground/blob/main/Hyperdimensional_Computing_Playground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperdimensional Computing (HDC) Playground\n",
        "\n",
        "This notebook is a hands-on exploration of [Hyperdimensional Computing (HDC)](https://en.wikipedia.org/wiki/Hyperdimensional_computing) created by [Samim](https://samim.io) for learning and experimentation purposes.\n",
        "\n",
        "**In this notebook, you'll find:**\n",
        "\n",
        "1. A very simple HDC toy example: A quick introduction to the basics of hyperdimensional computing.\n",
        "2. A simple HDC MNIST encoding and reconstruction example: Demonstrating how hypervectors can represent and reconstruct handwritten digits.\n",
        "3. A simple HDC TinyImageNet encoding and reconstruction example: Scaling up HDC to handle encoding and reconstruction of images, features and labels from a larger, more complex dataset.\n",
        "4. An intermediate HDC TinyImageNet encoding and reconstruction example, leveraging techniques like circular convolution, permutation, and sparse hypervectors for efficiency\n",
        "\n",
        "More HDC reading: [Simple HDC Intro](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012426) & [Classic HDC Paper](https://www.rctn.org/vs265/kanerva09-hyperdimensional.pdf)\n",
        "\n",
        "<br/>\n",
        "\n",
        "![HDC](https://samim.io/static/upload/journal.pcbi.1012426.g001.PNG)\n",
        "![HDC](https://samim.io/static/upload/journal.pcbi.1012426.g002.PNG)\n"
      ],
      "metadata": {
        "id": "fmhH3USYnPws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Toy Example\n",
        "\n",
        "This example introduces the basics of Hyperdimensional Computing (HDC). Here, we encode \"toys\" with specific attributes (e.g., \"red,\" \"hero,\" \"vehicle\") into high-dimensional vectors (hypervectors). By combining attributes through element-wise multiplication (binding), we create unique hypervectors representing different toys.\n",
        "\n",
        "Key concepts demonstrated:\n",
        "\n",
        "- Random High-Dimensional Vectors: Attributes like \"red\" and \"hero\" are represented as random binary hypervectors.\n",
        "- Binding and Encoding: Toys are encoded by combining their attributes using element-wise multiplication, a key operation in HDC.\n",
        "- Cosine Similarity for Queries: A query hypervector (e.g., \"red\") is compared to the database of toys to find similar items based on their encoded attributes.\n",
        "- Visualization with PCA: The high-dimensional hypervectors are reduced to 2D using PCA for visualization, showing how encoded toys cluster in space.\n",
        "\n",
        "This example shows how HDC can combine, store, and retrieve complex information efficiently while maintaining simplicity and robustness."
      ],
      "metadata": {
        "id": "pw2147NuHwWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 1: Create random high-dimensional vectors\n",
        "def create_random_vector(dimensions=10000):\n",
        "    return np.random.choice([-1, 1], size=dimensions)\n",
        "\n",
        "# Cosine similarity function\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "# Encode a toy by combining attributes\n",
        "def encode_toy(*attributes_to_combine):\n",
        "    encoded_vector = np.ones_like(attributes_to_combine[0])  # Start with a neutral vector\n",
        "    for attr in attributes_to_combine:\n",
        "        encoded_vector *= attr  # Element-wise multiplication (binding)\n",
        "    return encoded_vector\n",
        "\n",
        "# Query the database for similar vectors\n",
        "def query_database(query_vector, database, threshold=0.5):\n",
        "    results = []\n",
        "    for idx, toy in enumerate(database):\n",
        "        similarity = cosine_similarity(query_vector, toy)\n",
        "        if similarity > threshold:  # Matches exceed threshold\n",
        "            results.append((idx, similarity))\n",
        "    return results\n",
        "\n",
        "# Step 2: Create attributes as random high-dimensional vectors\n",
        "dimensions = 10000\n",
        "attributes = {\n",
        "    \"red\": create_random_vector(dimensions),\n",
        "    \"blue\": create_random_vector(dimensions),\n",
        "    \"action_figure\": create_random_vector(dimensions),\n",
        "    \"hero\": create_random_vector(dimensions),\n",
        "    \"vehicle\": create_random_vector(dimensions)\n",
        "}\n",
        "\n",
        "# Step 3: Encode some toys\n",
        "red_hero_action_figure = encode_toy(attributes[\"red\"], attributes[\"hero\"], attributes[\"action_figure\"])\n",
        "blue_vehicle = encode_toy(attributes[\"blue\"], attributes[\"vehicle\"])\n",
        "red_vehicle = encode_toy(attributes[\"red\"], attributes[\"vehicle\"])\n",
        "\n",
        "# Store toys in a database\n",
        "toy_database = [red_hero_action_figure, blue_vehicle, red_vehicle]\n",
        "\n",
        "# Step 4: Query for red toys\n",
        "red_query = attributes[\"red\"]\n",
        "matches = query_database(red_query, toy_database, threshold=0.5)\n",
        "\n",
        "# Step 5: Visualize the high-dimensional space using PCA\n",
        "# Reduce dimensionality for visualization\n",
        "pca = PCA(n_components=2)\n",
        "data_matrix = np.vstack([red_query] + toy_database)  # Combine query and database\n",
        "reduced_data = pca.fit_transform(data_matrix)\n",
        "\n",
        "# Plot the reduced vectors\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(reduced_data[1:, 0], reduced_data[1:, 1], color='blue', label='Toys')\n",
        "plt.scatter(reduced_data[0, 0], reduced_data[0, 1], color='red', label='Red Query', s=100)\n",
        "for i, (x, y) in enumerate(reduced_data[1:], start=1):\n",
        "    plt.text(x, y, f\"Toy {i}\", fontsize=10, color=\"blue\")\n",
        "\n",
        "plt.title(\"Visualization of High-Dimensional Vectors in 2D Space\")\n",
        "plt.xlabel(\"PCA Dimension 1\")\n",
        "plt.ylabel(\"PCA Dimension 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print query results\n",
        "for idx, similarity in matches:\n",
        "    print(f\"Toy {idx + 1} matches the query with similarity: {similarity:.2f}\")"
      ],
      "metadata": {
        "id": "bMBT_k2TizQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Hyperdimensional Computing for Image Reconstruction with MNIST\n",
        "\n",
        "This example applies Hyperdimensional Computing (HDC) to encode and reconstruct images from the MNIST dataset. Images are transformed into hypervectors using positional and brightness components, while a neural encoder-decoder network reconstructs the original images from these representations. The quality of reconstruction is assessed using metrics like Mean Squared Error (MSE) and Structural Similarity Index (SSIM). Latent space exploration visualizes smooth transitions between encoded representations, showcasing the interpretability and expressive power of HDC in capturing image structure."
      ],
      "metadata": {
        "id": "X1uFD8rG4xn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import imageio\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters\n",
        "dimensions = 20000  # Dimensionality of hypervectors\n",
        "image_shape = (28, 28)  # Image size\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "\n",
        "# Step 1: Random High-Dimensional Vectors\n",
        "def create_random_hv(dimensions):\n",
        "    \"\"\"Create a random hypervector.\"\"\"\n",
        "    return torch.randint(-1, 2, (dimensions,), device=device, dtype=torch.float32)\n",
        "\n",
        "def generate_position_hvs(image_shape, dimensions):\n",
        "    \"\"\"Generate positional hypervectors for each pixel.\"\"\"\n",
        "    return torch.stack([create_random_hv(dimensions) for _ in range(image_shape[0] * image_shape[1])])\n",
        "\n",
        "def encode_image_with_position_and_brightness(image, position_hvs, brightness_hv):\n",
        "    \"\"\"Encode an image using position and brightness hypervectors.\"\"\"\n",
        "    image_flat = image.flatten().to(device)  # Flatten the image\n",
        "    encoded_hv = torch.zeros(dimensions, device=device)\n",
        "    for i, intensity in enumerate(image_flat):\n",
        "        encoded_hv += intensity * position_hvs[i] * brightness_hv\n",
        "    return torch.sign(encoded_hv)\n",
        "\n",
        "# Generate position and brightness hypervectors\n",
        "position_hvs = generate_position_hvs(image_shape, dimensions)\n",
        "brightness_hv = create_random_hv(dimensions)\n",
        "\n",
        "# Step 2: Dataset Preparation\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_dataset_small = torch.utils.data.Subset(train_dataset, range(1000))\n",
        "test_dataset_small = torch.utils.data.Subset(test_dataset, range(100))\n",
        "\n",
        "train_loader = DataLoader(train_dataset_small, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset_small, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Step 3: Encoder and Decoder Networks\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dimensions):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, dimensions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dimensions):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(dimensions, 512)\n",
        "        self.fc2 = nn.Linear(512, 28 * 28)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        return x\n",
        "\n",
        "encoder = Encoder(dimensions).to(device)\n",
        "decoder = Decoder(dimensions).to(device)\n",
        "\n",
        "# Optimizers\n",
        "optimizer_encoder = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "optimizer_decoder = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "# Mixed Precision Training\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "# Step 4: Training Loop\n",
        "def train(encoder, decoder, train_loader, optimizer_encoder, optimizer_decoder, scaler):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_idx, (images, _) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass with mixed precision\n",
        "            with torch.cuda.amp.autocast(enabled=True):  # Explicitly enable autocast\n",
        "                encoded_hvs = encoder(images)\n",
        "                reconstructed_images = decoder(encoded_hvs)\n",
        "                loss = F.mse_loss(reconstructed_images, images)\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer_encoder.zero_grad()\n",
        "            optimizer_decoder.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Ensure gradients are valid before stepping\n",
        "            try:\n",
        "                scaler.step(optimizer_encoder)\n",
        "                scaler.step(optimizer_decoder)\n",
        "            except AssertionError:\n",
        "                print(\"Warning: Skipping optimizer step due to invalid gradients.\")\n",
        "                continue\n",
        "\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 10 == 0:  # Feedback every 10 batches\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "train(encoder, decoder, train_loader, optimizer_encoder, optimizer_decoder, scaler)\n",
        "\n",
        "# Step 5: Testing and Metrics\n",
        "def test(encoder, decoder, test_loader):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    mse_scores = []\n",
        "    ssim_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _ in test_loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Encode and decode\n",
        "            encoded_hvs = encoder(images)\n",
        "            reconstructed_images = decoder(encoded_hvs)\n",
        "\n",
        "            # Move to CPU for metrics\n",
        "            reconstructed_images_np = reconstructed_images.cpu().numpy()\n",
        "            images_np = images.cpu().numpy()\n",
        "\n",
        "            # Compute metrics\n",
        "            for i in range(images_np.shape[0]):\n",
        "                mse = mean_squared_error(images_np[i].flatten(), reconstructed_images_np[i].flatten())\n",
        "                mse_scores.append(mse)\n",
        "\n",
        "                ssim_score = ssim(\n",
        "                    images_np[i].squeeze(), reconstructed_images_np[i].squeeze(), data_range=1.0\n",
        "                )\n",
        "                ssim_scores.append(ssim_score)\n",
        "\n",
        "    print(f\"Average MSE: {np.mean(mse_scores):.4f}\")\n",
        "    print(f\"Average SSIM: {np.mean(ssim_scores):.4f}\")\n",
        "    return mse_scores, ssim_scores\n",
        "\n",
        "mse_scores, ssim_scores = test(encoder, decoder, test_loader)\n",
        "\n",
        "# Visualization: Original vs Reconstructed\n",
        "def visualize_reconstruction(encoder, decoder, test_loader):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    images, _ = next(iter(test_loader))\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoded_hvs = encoder(images)\n",
        "        reconstructed_images = decoder(encoded_hvs)\n",
        "\n",
        "    reconstructed_images_np = reconstructed_images.cpu().numpy()\n",
        "    images_np = images.cpu().numpy()\n",
        "\n",
        "    # Plot original and reconstructed images\n",
        "    for i in range(5):\n",
        "        plt.figure(figsize=(6, 3))\n",
        "        # Original\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(images_np[i].squeeze(), cmap=\"gray\")\n",
        "        plt.title(\"Original Image\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # Reconstructed\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.imshow(reconstructed_images_np[i].squeeze(), cmap=\"gray\")\n",
        "        plt.title(\"Reconstructed Image\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "visualize_reconstruction(encoder, decoder, test_loader)\n",
        "\n",
        "# Latent Space Exploration with GIF Display\n",
        "def walk_latent_space(encoder, decoder, test_loader):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        images, _ = next(iter(test_loader))\n",
        "        images = images.to(device)\n",
        "\n",
        "        hv_start = encoder(images[0:1])\n",
        "        hv_end = encoder(images[1:2])\n",
        "\n",
        "        steps = 20\n",
        "        latent_walk = [\n",
        "            hv_start + t / steps * (hv_end - hv_start)\n",
        "            for t in range(steps + 1)\n",
        "        ]\n",
        "\n",
        "        interpolated_images = [decoder(hv).squeeze(0).cpu().numpy() for hv in latent_walk]\n",
        "\n",
        "        # Save GIF without scaling\n",
        "        gif_filename = \"latent_walk.gif\"\n",
        "        with imageio.get_writer(gif_filename, mode=\"I\", duration=0.2, loop=0) as writer:  # `loop=0` ensures infinite looping\n",
        "            for img in interpolated_images:\n",
        "                img = (img * 255).astype(np.uint8).squeeze()\n",
        "                writer.append_data(img)\n",
        "\n",
        "        print(f\"Latent walk GIF saved as {gif_filename}\")\n",
        "\n",
        "        # Display the GIF inline in the notebook\n",
        "        display(Image(filename=gif_filename))\n",
        "\n",
        "\n",
        "walk_latent_space(encoder, decoder, test_loader)"
      ],
      "metadata": {
        "id": "EhwqscVZ283S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Hyperdimensional Computing for Image Reconstruction with TinyImageNet: Simple\n",
        "\n",
        "This script demonstrates Hyperdimensional Computing (HDC) for encoding and reconstructing images from the TinyImageNet dataset. It encodes images into hypervectors using positional and brightness components, and labels using class hypervectors, capturing hierarchical relationships. A neural decoder reconstructs the images, and quality is evaluated using MSE and SSIM. Latent space exploration and visualizations highlight HDC's interpretability and power."
      ],
      "metadata": {
        "id": "1caanp-UFQrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from IPython.display import display, Image\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Configuration\n",
        "use_subset = True  # Toggle this to use a subset for training and testing\n",
        "dimensions = 10000\n",
        "image_shape = (64, 64, 3)  # Image dimensions: height, width, channels\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "epochs = 150\n",
        "\n",
        "# Download and Extract TinyImageNet\n",
        "def download_and_extract_tinyimagenet():\n",
        "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "    dataset_folder = \"./tiny-imagenet-200\"\n",
        "    zip_filename = \"tiny-imagenet-200.zip\"\n",
        "\n",
        "    if not os.path.exists(dataset_folder):\n",
        "        print(\"Downloading TinyImageNet...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        with open(zip_filename, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Download complete. Extracting...\")\n",
        "        with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(\"./\")\n",
        "        print(\"Extraction complete.\")\n",
        "        os.remove(zip_filename)\n",
        "    else:\n",
        "        print(\"TinyImageNet already downloaded and extracted.\")\n",
        "\n",
        "download_and_extract_tinyimagenet()\n",
        "\n",
        "# Dataset Preparation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Use TinyImageNet Dataset\n",
        "train_dataset = datasets.ImageFolder(root='./tiny-imagenet-200/train', transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root='./tiny-imagenet-200/val', transform=transform)\n",
        "\n",
        "# Optional Subset\n",
        "if use_subset:\n",
        "    train_dataset = torch.utils.data.Subset(train_dataset, range(2000))\n",
        "    test_dataset = torch.utils.data.Subset(test_dataset, range(500))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Hyperdimensional Computing\n",
        "def create_random_hv(dimensions):\n",
        "    \"\"\"Create a random hypervector.\"\"\"\n",
        "    return torch.randint(-1, 2, (dimensions,), device=device, dtype=torch.float32)\n",
        "\n",
        "def generate_position_hvs(image_shape, dimensions):\n",
        "    \"\"\"Generate positional hypervectors for each pixel across all channels.\"\"\"\n",
        "    total_pixels = image_shape[0] * image_shape[1] * image_shape[2]  # Include channels\n",
        "    return torch.stack([create_random_hv(dimensions) for _ in range(total_pixels)])\n",
        "\n",
        "def encode_image_with_position_and_brightness_optimized(image, position_hvs, brightness_hv):\n",
        "    \"\"\"Optimized encoding to reduce memory footprint.\"\"\"\n",
        "    image_flat = image.flatten().to(device)\n",
        "    encoded_hv = torch.matmul(image_flat[:, None].T, position_hvs * brightness_hv).squeeze()\n",
        "    return torch.sign(encoded_hv)\n",
        "\n",
        "# Generate position and brightness hypervectors\n",
        "position_hvs = generate_position_hvs(image_shape, dimensions)\n",
        "brightness_hv = create_random_hv(dimensions)\n",
        "\n",
        "# Label Encoding\n",
        "num_classes = len(train_dataset.dataset.classes)\n",
        "label_hvs = torch.stack([create_random_hv(dimensions) for _ in range(num_classes)], dim=0)\n",
        "\n",
        "def encode_label_vectorized(labels):\n",
        "    \"\"\"Encode labels using precomputed label hypervectors (vectorized).\"\"\"\n",
        "    return label_hvs[labels]\n",
        "\n",
        "# Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dimensions):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(dimensions, 512)\n",
        "        self.fc2 = nn.Linear(512, 64 * 64 * 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        x = x.view(-1, 3, 64, 64)  # Reshape to image\n",
        "        return x\n",
        "\n",
        "decoder = Decoder(dimensions).to(device)\n",
        "optimizer_decoder = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "\n",
        "# train_and_test function\n",
        "def train_and_test(train_loader, test_loader):\n",
        "    for epoch in range(epochs):\n",
        "        decoder.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Encode images and labels\n",
        "            encoded_images = torch.stack([\n",
        "                encode_image_with_position_and_brightness_optimized(image, position_hvs, brightness_hv)\n",
        "                for image in images\n",
        "            ])\n",
        "            encoded_labels = encode_label_vectorized(labels)\n",
        "            encoded_hvs = encoded_images + encoded_labels\n",
        "\n",
        "            reconstructed_images = decoder(encoded_hvs)\n",
        "            loss = F.mse_loss(reconstructed_images, images)\n",
        "\n",
        "            optimizer_decoder.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_decoder.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "            # Free memory\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Enhanced Testing\n",
        "    decoder.eval()\n",
        "    mse_scores = []\n",
        "    ssim_scores = []\n",
        "    latent_walk_gif_filename = \"latent_space_exploration.gif\"\n",
        "    gif_images = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            encoded_images = torch.stack([\n",
        "                encode_image_with_position_and_brightness_optimized(image, position_hvs, brightness_hv)\n",
        "                for image in images\n",
        "            ])\n",
        "            encoded_labels = encode_label_vectorized(labels)\n",
        "            encoded_hvs = encoded_images + encoded_labels\n",
        "\n",
        "            reconstructed_images = decoder(encoded_hvs)\n",
        "\n",
        "            # Metrics and visualization\n",
        "            for i in range(images.size(0)):\n",
        "                original_image = images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "                reconstructed_image = reconstructed_images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "                mse = mean_squared_error(original_image.flatten(), reconstructed_image.flatten())\n",
        "                mse_scores.append(mse)\n",
        "\n",
        "                # Specify win_size and channel_axis for SSIM\n",
        "                similarity = ssim(\n",
        "                    original_image,\n",
        "                    reconstructed_image,\n",
        "                    multichannel=True,\n",
        "                    data_range=1.0,\n",
        "                    win_size=3,\n",
        "                    channel_axis=-1\n",
        "                )\n",
        "                ssim_scores.append(similarity)\n",
        "\n",
        "                # Save first few examples for GIF creation\n",
        "                if batch_idx == 0 and i < 5:\n",
        "                    gif_images.append((reconstructed_image * 255).astype(np.uint8))\n",
        "\n",
        "                # Display the first batch for class-wise reconstruction\n",
        "                if batch_idx == 0 and i < 5:\n",
        "                    plt.figure(figsize=(6, 3))\n",
        "                    plt.subplot(1, 2, 1)\n",
        "                    plt.imshow(original_image)\n",
        "                    plt.title(\"Original Image\")\n",
        "                    plt.axis(\"off\")\n",
        "\n",
        "                    plt.subplot(1, 2, 2)\n",
        "                    plt.imshow(reconstructed_image)\n",
        "                    plt.title(\"Reconstructed Image\")\n",
        "                    plt.axis(\"off\")\n",
        "                    plt.show()\n",
        "\n",
        "            if batch_idx == 0:\n",
        "                break\n",
        "\n",
        "    print(f\"Average MSE: {np.mean(mse_scores):.4f}\")\n",
        "    print(f\"Average SSIM: {np.mean(ssim_scores):.4f}\")\n",
        "\n",
        "    # Save latent space exploration as GIF\n",
        "    try:\n",
        "        import imageio\n",
        "        imageio.mimsave(latent_walk_gif_filename, gif_images, fps=2)\n",
        "        print(f\"Latent space exploration GIF saved as {latent_walk_gif_filename}\")\n",
        "    except ImportError:\n",
        "        print(\"Install imageio for GIF generation.\")\n",
        "\n",
        "# Run the training and testing\n",
        "train_and_test(train_loader, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "id": "rBnIH4wb_9L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Hyperdimensional Computing for Image Reconstruction with TinyImageNet: Complex\n",
        "\n",
        "This script implements a Hyperdimensional Computing (HDC)-based autoencoder for image reconstruction using the TinyImageNet dataset. It encodes images into high-dimensional hypervectors by combining positional and brightness information, leveraging techniques like circular convolution, permutation, and sparse hypervectors for efficiency. The encoded hypervectors are then decoded back into images using a neural network. The script includes data augmentation, dropout regularization, and mixed precision training to improve robustness and performance. It evaluates the model using metrics like MSE, SSIM, and PSNR, and visualizes the results by comparing original and reconstructed images, as well as generating a GIF of the reconstructions. The entire pipeline is optimized for speed and scalability, making it suitable for exploring HDC principles in image processing tasks."
      ],
      "metadata": {
        "id": "BnSCKOdUlUiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from tqdm import tqdm\n",
        "import imageio\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Configuration\n",
        "use_subset = True  # Toggle this to use a subset for training and testing\n",
        "dimensions = 5000  # Reduced dimensionality for faster computation\n",
        "image_shape = (64, 64, 3)  # Image dimensions: height, width, channels\n",
        "batch_size = 16\n",
        "learning_rate = 0.001\n",
        "epochs = 30  # Reduced epochs for faster testing\n",
        "noise_level = 0.1  # Noise for robustness\n",
        "brightness_levels = 256  # 8-bit grayscale\n",
        "sparsity = 0.1  # Sparsity level for hypervectors\n",
        "\n",
        "# Download and Extract TinyImageNet\n",
        "def download_and_extract_tinyimagenet():\n",
        "    url = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
        "    dataset_folder = \"./tiny-imagenet-200\"\n",
        "    zip_filename = \"tiny-imagenet-200.zip\"\n",
        "\n",
        "    if not os.path.exists(dataset_folder):\n",
        "        print(\"Downloading TinyImageNet...\")\n",
        "        response = requests.get(url, stream=True)\n",
        "        with open(zip_filename, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        print(\"Download complete. Extracting...\")\n",
        "        with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
        "            zip_ref.extractall(\"./\")\n",
        "        print(\"Extraction complete.\")\n",
        "        os.remove(zip_filename)\n",
        "    else:\n",
        "        print(\"TinyImageNet already downloaded and extracted.\")\n",
        "\n",
        "download_and_extract_tinyimagenet()\n",
        "\n",
        "# Dataset Preparation with Data Augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.RandomHorizontalFlip(),  # Data augmentation\n",
        "    transforms.RandomRotation(10),  # Data augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Use TinyImageNet Dataset\n",
        "train_dataset = datasets.ImageFolder(root='./tiny-imagenet-200/train', transform=transform)\n",
        "test_dataset = datasets.ImageFolder(root='./tiny-imagenet-200/val', transform=transform)\n",
        "\n",
        "# Optional Subset\n",
        "if use_subset:\n",
        "    train_dataset = torch.utils.data.Subset(train_dataset, range(500))  # Smaller subset\n",
        "    test_dataset = torch.utils.data.Subset(test_dataset, range(100))\n",
        "\n",
        "# Increase number of workers in DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "# Hyperdimensional Computing\n",
        "def create_random_hv(dimensions):\n",
        "    \"\"\"Create a random hypervector with ternary values {-1, 0, 1}.\"\"\"\n",
        "    return torch.randint(-1, 2, (dimensions,), device=device, dtype=torch.float32)\n",
        "\n",
        "def create_sparse_hv(dimensions, sparsity=0.1):\n",
        "    \"\"\"Create a sparse hypervector with a given sparsity level.\"\"\"\n",
        "    hv = torch.zeros(dimensions, device=device)\n",
        "    nonzero_indices = torch.randperm(dimensions)[:int(dimensions * sparsity)]\n",
        "    hv[nonzero_indices] = torch.randint(-1, 2, (int(dimensions * sparsity),), device=device, dtype=torch.float32)\n",
        "    return hv\n",
        "\n",
        "def circular_convolution(hv1, hv2):\n",
        "    \"\"\"Circular convolution for binding hypervectors.\"\"\"\n",
        "    return torch.fft.irfft(torch.fft.rfft(hv1) * torch.fft.rfft(hv2), n=dimensions)\n",
        "\n",
        "def permute(hv, shift):\n",
        "    \"\"\"Cyclic shift (permutation) of a hypervector.\"\"\"\n",
        "    return torch.roll(hv, shifts=shift, dims=0)\n",
        "\n",
        "def generate_position_hvs(image_shape, dimensions):\n",
        "    \"\"\"Generate positional hypervectors using random permutations.\"\"\"\n",
        "    base_hv = create_sparse_hv(dimensions, sparsity)\n",
        "    total_pixels = image_shape[0] * image_shape[1] * image_shape[2]  # Include channels\n",
        "    position_hvs = torch.stack([permute(base_hv, i) for i in range(total_pixels)])\n",
        "    return position_hvs\n",
        "\n",
        "def encode_batch_with_position_and_brightness(images, position_hvs, brightness_hvs):\n",
        "    \"\"\"Encode a batch of images using sparse matrix operations.\"\"\"\n",
        "    batch_size, channels, height, width = images.shape\n",
        "    images_flat = images.view(batch_size, -1).to(device)  # Flatten images\n",
        "    intensity_indices = ((images_flat + 1) * (brightness_levels - 1) / 2).long()  # Scale to brightness levels\n",
        "    brightness_hvs_selected = brightness_hvs[intensity_indices]  # Select brightness hypervectors\n",
        "    encoded_hvs = torch.sum(brightness_hvs_selected * position_hvs, dim=1)  # Vectorized sum\n",
        "    return torch.sign(encoded_hvs)  # Quantize to ternary\n",
        "\n",
        "# Generate position and brightness hypervectors\n",
        "position_hvs = generate_position_hvs(image_shape, dimensions)\n",
        "brightness_hvs = torch.stack([create_sparse_hv(dimensions, sparsity) for _ in range(brightness_levels)])\n",
        "\n",
        "# Label Encoding\n",
        "num_classes = len(train_dataset.dataset.classes)\n",
        "label_hvs = torch.stack([create_sparse_hv(dimensions, sparsity) for _ in range(num_classes)], dim=0)\n",
        "\n",
        "def encode_label_vectorized(labels):\n",
        "    \"\"\"Encode labels using precomputed label hypervectors (vectorized).\"\"\"\n",
        "    return label_hvs[labels]\n",
        "\n",
        "# Hybrid Decoder with Dropout\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dimensions):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(dimensions, 512)\n",
        "        self.dropout = nn.Dropout(0.5)  # Add dropout\n",
        "        self.fc2 = nn.Linear(512, 64 * 64 * 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Apply dropout\n",
        "        x = torch.tanh(self.fc2(x))  # Output in [-1, 1]\n",
        "        x = x.view(-1, 3, 64, 64)  # Reshape to image\n",
        "        return x\n",
        "\n",
        "decoder = Decoder(dimensions).to(device)\n",
        "optimizer_decoder = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer_decoder, step_size=10, gamma=0.5)  # Learning rate scheduler\n",
        "scaler = GradScaler()  # For mixed precision training\n",
        "\n",
        "# Training and Testing\n",
        "def train_and_test(train_loader, test_loader):\n",
        "    for epoch in range(epochs):\n",
        "        decoder.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Encode images and labels\n",
        "            with autocast():\n",
        "                encoded_images = encode_batch_with_position_and_brightness(images, position_hvs, brightness_hvs)\n",
        "                encoded_labels = encode_label_vectorized(labels)\n",
        "                encoded_hvs = circular_convolution(encoded_images, encoded_labels)  # Use circular convolution\n",
        "\n",
        "                # Add noise for robustness\n",
        "                encoded_hvs = encoded_hvs + torch.randn_like(encoded_hvs) * noise_level\n",
        "\n",
        "                # Forward pass\n",
        "                reconstructed_images = decoder(encoded_hvs)\n",
        "                loss = F.mse_loss(reconstructed_images, images)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer_decoder.zero_grad()\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer_decoder)\n",
        "            scaler.update()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Free memory\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        scheduler.step()  # Update learning rate\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Average Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Enhanced Testing\n",
        "    decoder.eval()\n",
        "    mse_scores = []\n",
        "    ssim_scores = []\n",
        "    psnr_scores = []\n",
        "    gif_images = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Encode images and labels\n",
        "            encoded_images = encode_batch_with_position_and_brightness(images, position_hvs, brightness_hvs)\n",
        "            encoded_labels = encode_label_vectorized(labels)\n",
        "            encoded_hvs = circular_convolution(encoded_images, encoded_labels)  # Use circular convolution\n",
        "\n",
        "            # Reconstruct images\n",
        "            reconstructed_images = decoder(encoded_hvs)\n",
        "\n",
        "            # Metrics and visualization\n",
        "            for i in range(images.size(0)):\n",
        "                original_image = images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "                reconstructed_image = reconstructed_images[i].cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "                mse = mean_squared_error(original_image.flatten(), reconstructed_image.flatten())\n",
        "                mse_scores.append(mse)\n",
        "\n",
        "                # Specify win_size and channel_axis for SSIM\n",
        "                similarity = ssim(\n",
        "                    original_image,\n",
        "                    reconstructed_image,\n",
        "                    multichannel=True,\n",
        "                    data_range=2.0,  # Range is 2 for [-1, 1]\n",
        "                    win_size=3,\n",
        "                    channel_axis=-1\n",
        "                )\n",
        "                ssim_scores.append(similarity)\n",
        "\n",
        "                # Compute PSNR\n",
        "                psnr = -10 * np.log10(mse)\n",
        "                psnr_scores.append(psnr)\n",
        "\n",
        "                # Save first few examples for GIF creation\n",
        "                if batch_idx == 0 and i < 5:\n",
        "                    gif_images.append(((reconstructed_image + 1) * 127.5).astype(np.uint8))  # Scale to [0, 255]\n",
        "\n",
        "                # Display the first batch for class-wise reconstruction\n",
        "                if batch_idx == 0 and i < 5:\n",
        "                    plt.figure(figsize=(6, 3))\n",
        "                    plt.subplot(1, 2, 1)\n",
        "                    plt.imshow((original_image + 1) / 2)  # Scale to [0, 1] for display\n",
        "                    plt.title(\"Original Image\")\n",
        "                    plt.axis(\"off\")\n",
        "\n",
        "                    plt.subplot(1, 2, 2)\n",
        "                    plt.imshow((reconstructed_image + 1) / 2)  # Scale to [0, 1] for display\n",
        "                    plt.title(\"Reconstructed Image\")\n",
        "                    plt.axis(\"off\")\n",
        "                    plt.show()\n",
        "\n",
        "            if batch_idx == 0:\n",
        "                break\n",
        "\n",
        "    print(f\"Average MSE: {np.mean(mse_scores):.4f}\")\n",
        "    print(f\"Average SSIM: {np.mean(ssim_scores):.4f}\")\n",
        "    print(f\"Average PSNR: {np.mean(psnr_scores):.4f}\")\n",
        "\n",
        "    # Save latent space exploration as GIF\n",
        "    latent_walk_gif_filename = \"latent_space_exploration.gif\"\n",
        "    imageio.mimsave(latent_walk_gif_filename, gif_images, fps=2)\n",
        "    print(f\"Latent space exploration GIF saved as {latent_walk_gif_filename}\")\n",
        "\n",
        "# Run the training and testing\n",
        "train_and_test(train_loader, test_loader)"
      ],
      "metadata": {
        "id": "Pbl3lFj1jsXw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}